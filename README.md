# RL - 90 days 


## Week #1 - Introduction to Reinforcement Learning
- D1 - [Reinforcement Learning - Sutton, Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) - Chapter 1 - Introduction
- D2 - [DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]](https://www.youtube.com/watch?v=TCCjZe0y4Qc)
- D3 - [ Introduction to Reinforcement Learning - David Silver](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/intro_rl.pdf)
- D4 - [Deep Reinforcement Learning: Pong from Pixels](https://karpathy.github.io/2016/05/31/rl/)

### After this week you should know

After this week, you should understand the **core idea of Reinforcement Learning** as a framework for sequential decision-making under uncertainty. In particular, you should know:
* What Reinforcement Learning is and how it differs from supervised and unsupervised learning
* The standard **RL terminology**: agent, environment, state, action, reward, policy, return, episode
* How RL problems are formalized as **Markov Decision Processes (MDPs)**
* The concept of **return** and why discounting future rewards is important
* The difference between **episodic vs continuing tasks**
* What a **policy** is and deterministic vs stochastic policies
* The intuition behind **value functions** (state-value and action-value)
* The **exploration vs exploitation** trade-off
* Why RL is hard in practice (credit assignment, delayed rewards, instability)
* High-level intuition of **deep reinforcement learning**, where neural networks replace tabular representations
* How an end-to-end RL system (e.g. Atari Pong from pixels) looks: perception → policy → reward → learning loop

### Exam questions (self-check)
Use these questions to verify whether you really understand the material:

1. What makes Reinforcement Learning fundamentally different from supervised learning?
2. Define: state, action, reward, policy, return.
3. What is a Markov Decision Process? What assumptions does it make?
4. Why do we use a discount factor? What happens if γ = 0 or γ → 1?
5. What is the difference between episodic and continuing tasks?
6. What is the exploration–exploitation dilemma? Give a concrete example.
7. What is a policy? What is the difference between deterministic and stochastic policies?
8. What does a value function represent intuitively?
9. How is an action-value function different from a state-value function?
10. Why are value functions useful for learning good policies?
11. Why is Reinforcement Learning considered unstable or difficult to train?
12. What problems arise when using neural networks in RL instead of tables?
13. In the “Pong from Pixels” setup, what are:
    * the state?
    * the action?
    * the reward?
14. Why is it significant that the agent learns directly from pixels?




## Week #2

## Week #3

## Week #4

## Week #5

## Week #6


