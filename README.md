# Reinforcement Learning 90 Days Challenge - 2026 (new and curated)

This repository documents a **90-day, hands-on journey into Reinforcement Learning (RL)** — from fundamentals to modern deep RL methods. The goal of this challenge is not just to *read* about RL, but to **build intuition, implement core algorithms from scratch, and develop practical understanding** of how and why they work. 

The challenge is structured week by week. Each week focuses on a specific theme, combining:

* **High-quality theoretical resources** (classic textbooks, lectures, papers)
* **Practical implementations** (from tabular methods to deep RL)
* **Self-check questions** to verify real understanding, not just familiarity with terms

This repository serves as:
* a **learning log**,
* a **reference of curated RL resources**,
* and a **codebase of minimal, readable implementations**.

The emphasis is on:
* understanding RL fundamentals deeply before scaling up,
* avoiding “black-box” implementations,
* and building algorithms step by step, with clear assumptions and trade-offs.

By the end of the 90 days, the goal is to be comfortable reasoning about RL problems, reading RL papers, and implementing key algorithms independently.

> *Learn slowly. Implement everything. Question every assumption.*

## Week #1 - Introduction to Reinforcement Learning
- D1 - [Reinforcement Learning - Sutton, Barto](https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf) - Chapter 1 - Introduction
- D2 - [DeepMind x UCL RL Lecture Series - Introduction to Reinforcement Learning [1/13]](https://www.youtube.com/watch?v=TCCjZe0y4Qc)
- D3 - [Introduction to Reinforcement Learning - David Silver](https://davidstarsilver.wordpress.com/wp-content/uploads/2025/04/intro_rl.pdf) snd video [RL Course by David Silver - Lecture 1: Introduction to Reinforcement Learning](https://www.youtube.com/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ)
- D4 - [Deep Reinforcement Learning: Pong from Pixels](https://karpathy.github.io/2016/05/31/rl/)

### After this week you should know

* What Reinforcement Learning is and how it differs from supervised and unsupervised learning
* The standard **RL terminology**: agent, environment, state, action, reward, policy, return, episode
* How RL problems are formalized as **Markov Decision Processes (MDPs)**
* The concept of **return** and why discounting future rewards is important
* The difference between **episodic vs continuing tasks**
* What a **policy** is and deterministic vs stochastic policies
* The intuition behind **value functions** (state-value and action-value)
* The **exploration vs exploitation** trade-off
* Why RL is hard in practice (credit assignment, delayed rewards, instability)
* High-level intuition of **deep reinforcement learning**, where neural networks replace tabular representations
* How an end-to-end RL system (e.g. Atari Pong from pixels) looks: perception → policy → reward → learning loop

### Exam questions (self-check)
Use these questions to verify whether you really understand the material:

1. What makes Reinforcement Learning fundamentally different from supervised learning?
2. Define: state, action, reward, policy, return.
3. What is a Markov Decision Process? What assumptions does it make?
4. Why do we use a discount factor? What happens if γ = 0 or γ → 1?
5. What is the difference between episodic and continuing tasks?
6. What is the exploration–exploitation dilemma? Give a concrete example.
7. What is a policy? What is the difference between deterministic and stochastic policies?
8. What does a value function represent intuitively?
9. How is an action-value function different from a state-value function?
10. Why are value functions useful for learning good policies?
11. Why is Reinforcement Learning considered unstable or difficult to train?
12. What problems arise when using neural networks in RL instead of tables?
13. In the “Pong from Pixels” setup, what are:
    * the state?
    * the action?
    * the reward?
14. Why is it significant that the agent learns directly from pixels?




## Week #2

## Week #3

## Week #4

## Week #5

## Week #6


